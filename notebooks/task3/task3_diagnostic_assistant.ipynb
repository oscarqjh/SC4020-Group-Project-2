{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: AI Diagnostic Assistant\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook, ensure that all necessary dependencies are installed and required directories are created by executing the setup script:\n",
    "\n",
    "```bash\n",
    "bash scripts/setup.sh\n",
    "```\n",
    "\n",
    "Alternatively, manually install dependencies with `pip install -r requirements.txt` and create the `outputs/models/` and `outputs/vectorstore/` directories. For detailed setup instructions, refer to the **Setup** section in `docs/task3_implementation_plan.md`.\n",
    "\n",
    "## Objective\n",
    "The goal of this task is to build an AI-powered diagnostic assistant that can help users understand their symptoms and provide insights about cancer-related health data. The assistant integrates two tools:\n",
    "- **Tool 1**: Symptom Checker - Uses RAG (Retrieval-Augmented Generation) with ChromaDB to provide disease information and precautions based on user symptoms\n",
    "- **Tool 2**: Cancer Analysis - Analyzes breast cancer data patterns using sequential pattern mining insights\n",
    "\n",
    "## Overview\n",
    "This notebook implements:\n",
    "1. ML Model Training & Vocabulary Export (Phase 1)\n",
    "2. Knowledge Base Setup with ChromaDB (Phase 2)\n",
    "3. Demonstration of the ProjectAssistant (Phase 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: ML Model Training & Vocabulary Export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup and Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (4920, 18)\n",
      "\n",
      "First few rows:\n",
      "            Disease   Symptom_1              Symptom_2              Symptom_3  \\\n",
      "0  Fungal infection     itching              skin_rash   nodal_skin_eruptions   \n",
      "1  Fungal infection   skin_rash   nodal_skin_eruptions    dischromic _patches   \n",
      "2  Fungal infection     itching   nodal_skin_eruptions    dischromic _patches   \n",
      "3  Fungal infection     itching              skin_rash    dischromic _patches   \n",
      "4  Fungal infection     itching              skin_rash   nodal_skin_eruptions   \n",
      "\n",
      "              Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8 Symptom_9  \\\n",
      "0   dischromic _patches       NaN       NaN       NaN       NaN       NaN   \n",
      "1                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "2                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "3                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "4                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "  Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "  Symptom_16 Symptom_17  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "Column names:\n",
      "['Disease', 'Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4', 'Symptom_5', 'Symptom_6', 'Symptom_7', 'Symptom_8', 'Symptom_9', 'Symptom_10', 'Symptom_11', 'Symptom_12', 'Symptom_13', 'Symptom_14', 'Symptom_15', 'Symptom_16', 'Symptom_17']\n",
      "\n",
      "==================================================\n",
      "Preprocessing Results\n",
      "==================================================\n",
      "\n",
      "Sample symptoms_text:\n",
      "itching skin_rash nodal_skin_eruptions dischromic_patches\n",
      "\n",
      "Number of unique diseases: 41\n",
      "\n",
      "Example - Disease: Fungal infection, Symptoms: itching skin_rash nodal_skin_eruptions dischromic_patches\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "DATA_PATH = project_root / 'data' / 'dataset.csv'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Combine symptom columns into a single text field\n",
    "symptom_cols = [f'Symptom_{i}' for i in range(1, 18)]\n",
    "\n",
    "# Define function to clean string values only (treat non-strings as None)\n",
    "def clean_symptom_value(value):\n",
    "    \"\"\"Clean symptom value: trim and normalize underscores for strings only, leave non-strings as None.\"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return None\n",
    "    # Strip whitespace and normalize underscores (remove spaces around underscores, normalize multiple underscores)\n",
    "    import re\n",
    "    cleaned = value.strip()\n",
    "    cleaned = re.sub(r'\\s+_\\s+', '_', cleaned)  # Remove spaces around underscores\n",
    "    cleaned = re.sub(r'\\s+_', '_', cleaned)  # Remove spaces before underscores\n",
    "    cleaned = re.sub(r'_\\s+', '_', cleaned)  # Remove spaces after underscores\n",
    "    cleaned = cleaned.strip('_')  # Remove leading/trailing underscores\n",
    "    return cleaned if cleaned else None\n",
    "\n",
    "# Apply cleaning function to each symptom column (only processes strings, leaves NaN as None)\n",
    "for col in symptom_cols:\n",
    "    df[col] = df[col].apply(clean_symptom_value)\n",
    "\n",
    "# Build symptoms_text from symptom_cols by selecting only non-null entries (without prior astype(str))\n",
    "def build_symptoms_text(row):\n",
    "    \"\"\"Build symptoms_text from symptom columns, selecting only non-null entries without converting NaN to strings.\"\"\"\n",
    "    symptoms = []\n",
    "    for val in row[symptom_cols]:\n",
    "        if pd.notna(val) and val is not None and isinstance(val, str) and val.strip():\n",
    "            symptoms.append(val)\n",
    "    return ' '.join(symptoms)\n",
    "\n",
    "df['symptoms_text'] = df.apply(build_symptoms_text, axis=1)\n",
    "\n",
    "# Clean up multiple spaces in symptoms_text\n",
    "df['symptoms_text'] = df['symptoms_text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Drop rows with empty symptoms_text\n",
    "df = df[df['symptoms_text'].notna() & (df['symptoms_text'].str.strip() != '')].copy()\n",
    "\n",
    "# Normalize Disease column: strip whitespace\n",
    "df['Disease'] = df['Disease'].str.strip()\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['symptoms_text']\n",
    "y = df['Disease']\n",
    "\n",
    "# Display preprocessing results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Preprocessing Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSample symptoms_text:\")\n",
    "print(X.iloc[0])\n",
    "print(f\"\\nNumber of unique diseases: {y.nunique()}\")\n",
    "print(f\"\\nExample - Disease: {y.iloc[0]}, Symptoms: {X.iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Model training complete.\n",
      "\n",
      "Model Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                                         precision    recall  f1-score   support\n",
      "\n",
      "(vertigo) Paroymsal  Positional Vertigo       1.00      1.00      1.00        18\n",
      "                                   AIDS       1.00      1.00      1.00        30\n",
      "                                   Acne       1.00      1.00      1.00        24\n",
      "                    Alcoholic hepatitis       1.00      1.00      1.00        25\n",
      "                                Allergy       1.00      1.00      1.00        24\n",
      "                              Arthritis       1.00      1.00      1.00        23\n",
      "                       Bronchial Asthma       1.00      1.00      1.00        33\n",
      "                   Cervical spondylosis       1.00      1.00      1.00        23\n",
      "                            Chicken pox       1.00      1.00      1.00        21\n",
      "                    Chronic cholestasis       1.00      1.00      1.00        15\n",
      "                            Common Cold       1.00      1.00      1.00        23\n",
      "                                 Dengue       1.00      1.00      1.00        26\n",
      "                               Diabetes       1.00      1.00      1.00        21\n",
      "           Dimorphic hemmorhoids(piles)       1.00      1.00      1.00        29\n",
      "                          Drug Reaction       1.00      1.00      1.00        24\n",
      "                       Fungal infection       1.00      1.00      1.00        19\n",
      "                                   GERD       1.00      1.00      1.00        28\n",
      "                        Gastroenteritis       1.00      1.00      1.00        25\n",
      "                           Heart attack       1.00      1.00      1.00        23\n",
      "                            Hepatitis B       1.00      1.00      1.00        27\n",
      "                            Hepatitis C       1.00      1.00      1.00        26\n",
      "                            Hepatitis D       1.00      1.00      1.00        23\n",
      "                            Hepatitis E       1.00      1.00      1.00        29\n",
      "                           Hypertension       1.00      1.00      1.00        25\n",
      "                        Hyperthyroidism       1.00      1.00      1.00        24\n",
      "                           Hypoglycemia       1.00      1.00      1.00        26\n",
      "                         Hypothyroidism       1.00      1.00      1.00        21\n",
      "                               Impetigo       1.00      1.00      1.00        24\n",
      "                               Jaundice       1.00      1.00      1.00        19\n",
      "                                Malaria       1.00      1.00      1.00        22\n",
      "                               Migraine       1.00      1.00      1.00        25\n",
      "                        Osteoarthristis       1.00      1.00      1.00        22\n",
      "           Paralysis (brain hemorrhage)       1.00      1.00      1.00        24\n",
      "                    Peptic ulcer diseae       1.00      1.00      1.00        17\n",
      "                              Pneumonia       1.00      1.00      1.00        28\n",
      "                              Psoriasis       1.00      1.00      1.00        22\n",
      "                           Tuberculosis       1.00      1.00      1.00        25\n",
      "                                Typhoid       1.00      1.00      1.00        19\n",
      "                Urinary tract infection       1.00      1.00      1.00        26\n",
      "                         Varicose veins       1.00      1.00      1.00        22\n",
      "                            hepatitis A       1.00      1.00      1.00        34\n",
      "\n",
      "                               accuracy                           1.00       984\n",
      "                              macro avg       1.00      1.00      1.00       984\n",
      "                           weighted avg       1.00      1.00      1.00       984\n",
      "\n",
      "\n",
      "Vocabulary exported to: /Users/bytedance/GitHub/SC4020-Group-Project-2/outputs/models/symptom_vocabulary.json\n",
      "Vocabulary size: 133 unique symptom tokens\n",
      "Trained model saved to: /Users/bytedance/GitHub/SC4020-Group-Project-2/outputs/models/disease_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the ML pipeline\n",
    "disease_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "disease_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = disease_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Export the vocabulary\n",
    "vocabulary = disease_pipeline.named_steps['vectorizer'].get_feature_names_out()\n",
    "vocabulary_list = vocabulary.tolist()\n",
    "\n",
    "vocab_path = project_root / 'outputs' / 'models' / 'symptom_vocabulary.json'\n",
    "vocab_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(vocab_path, 'w') as f:\n",
    "    json.dump(vocabulary_list, f, indent=2)\n",
    "\n",
    "print(f\"\\nVocabulary exported to: {vocab_path}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary_list)} unique symptom tokens\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = project_root / 'outputs' / 'models' / 'disease_model.pkl'\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(disease_pipeline, model_path)\n",
    "\n",
    "print(f\"Trained model saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Knowledge Base Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup and Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer model loaded successfully.\n",
      "Embedding dimension: 384\n",
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Define project_root for Phase 2 (allows Phase 2 to run independently)\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"SentenceTransformer model loaded successfully.\")\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Disease Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions shape: (41, 2)\n",
      "          Disease                                        Description\n",
      "0   Drug Reaction  An adverse drug reaction (ADR) is an injury ca...\n",
      "1         Malaria  An infectious disease caused by protozoan para...\n",
      "2         Allergy  An allergy is an immune system response to a f...\n",
      "3  Hypothyroidism  Hypothyroidism, also called underactive thyroi...\n",
      "4       Psoriasis  Psoriasis is a common skin disorder that forms...\n",
      "\n",
      "Precautions shape: (41, 5)\n",
      "          Disease                      Precaution_1  \\\n",
      "0   Drug Reaction                   stop irritation   \n",
      "1         Malaria          Consult nearest hospital   \n",
      "2         Allergy                    apply calamine   \n",
      "3  Hypothyroidism                     reduce stress   \n",
      "4       Psoriasis  wash hands with warm soapy water   \n",
      "\n",
      "                   Precaution_2        Precaution_3  \\\n",
      "0      consult nearest hospital    stop taking drug   \n",
      "1               avoid oily food  avoid non veg food   \n",
      "2       cover area with bandage                 NaN   \n",
      "3                      exercise         eat healthy   \n",
      "4  stop bleeding using pressure      consult doctor   \n",
      "\n",
      "                  Precaution_4  \n",
      "0                    follow up  \n",
      "1           keep mosquitos out  \n",
      "2  use ice to compress itching  \n",
      "3             get proper sleep  \n",
      "4                   salt baths  \n",
      "\n",
      "After normalization:\n",
      "Unique diseases in descriptions: 41\n",
      "Unique diseases in precautions: 41\n",
      "‚úì All diseases match between descriptions and precautions files.\n"
     ]
    }
   ],
   "source": [
    "# Load disease descriptions and precautions\n",
    "DESCRIPTION_PATH = project_root / 'data' / 'symptom_Description.csv'\n",
    "PRECAUTION_PATH = project_root / 'data' / 'symptom_precaution.csv'\n",
    "\n",
    "# Load disease descriptions\n",
    "descriptions_df = pd.read_csv(DESCRIPTION_PATH)\n",
    "print(f\"Descriptions shape: {descriptions_df.shape}\")\n",
    "print(descriptions_df.head())\n",
    "\n",
    "# Load disease precautions\n",
    "precautions_df = pd.read_csv(PRECAUTION_PATH)\n",
    "print(f\"\\nPrecautions shape: {precautions_df.shape}\")\n",
    "print(precautions_df.head())\n",
    "\n",
    "# Data cleaning: strip whitespace from Disease column\n",
    "descriptions_df['Disease'] = descriptions_df['Disease'].str.strip()\n",
    "precautions_df['Disease'] = precautions_df['Disease'].str.strip()\n",
    "\n",
    "# Normalize disease names using correction map to fix typos and mismatches\n",
    "def normalize_disease_name(name):\n",
    "    \"\"\"Normalize disease names to fix typos and inconsistencies.\"\"\"\n",
    "    # Correction map for known typos/mismatches\n",
    "    correction_map = {\n",
    "        'hemmorhoids': 'hemorrhoids',  # Fix typo: hemmorhoids -> hemorrhoids\n",
    "        'Paroymsal': 'Paroxysmal',  # Fix typo: Paroymsal -> Paroxysmal\n",
    "    }\n",
    "    \n",
    "    normalized = name\n",
    "    for typo, correct in correction_map.items():\n",
    "        if typo in normalized:\n",
    "            normalized = normalized.replace(typo, correct)\n",
    "    return normalized\n",
    "\n",
    "# Apply normalization to both dataframes\n",
    "descriptions_df['Disease'] = descriptions_df['Disease'].apply(normalize_disease_name)\n",
    "precautions_df['Disease'] = precautions_df['Disease'].apply(normalize_disease_name)\n",
    "\n",
    "# Verify data alignment after normalization\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"Unique diseases in descriptions: {descriptions_df['Disease'].nunique()}\")\n",
    "print(f\"Unique diseases in precautions: {precautions_df['Disease'].nunique()}\")\n",
    "\n",
    "desc_diseases = set(descriptions_df['Disease'])\n",
    "prec_diseases = set(precautions_df['Disease'])\n",
    "mismatches_desc = desc_diseases - prec_diseases\n",
    "mismatches_prec = prec_diseases - desc_diseases\n",
    "\n",
    "if mismatches_desc:\n",
    "    print(f\"Diseases only in descriptions: {mismatches_desc}\")\n",
    "if mismatches_prec:\n",
    "    print(f\"Diseases only in precautions: {mismatches_prec}\")\n",
    "if not mismatches_desc and not mismatches_prec:\n",
    "    print(\"‚úì All diseases match between descriptions and precautions files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Initialize ChromaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 2] Step 3: Initializing ChromaDB...\n",
      "Vectorstore directory: /Users/bytedance/GitHub/SC4020-Group-Project-2/outputs/vectorstore/chroma_db\n",
      "‚úì ChromaDB persistent client initialized.\n",
      "  Cleared existing 'disease_info' collection.\n",
      "‚úì Collection 'disease_info' created. Current document count: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Phase 2] Step 3: Initializing ChromaDB...\")\n",
    "VECTORSTORE_PATH = project_root / 'outputs' / 'vectorstore' / 'chroma_db'\n",
    "\n",
    "# Create the vectorstore directory\n",
    "VECTORSTORE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Vectorstore directory: {VECTORSTORE_PATH}\")\n",
    "\n",
    "# Initialize ChromaDB persistent client\n",
    "chroma_client = chromadb.PersistentClient(path=str(VECTORSTORE_PATH))\n",
    "print(\"‚úì ChromaDB persistent client initialized.\")\n",
    "\n",
    "# Define embedding function class for ChromaDB 1.3.0\n",
    "class SentenceTransformerEmbeddingFunction:\n",
    "    \"\"\"Embedding function class that wraps SentenceTransformer for ChromaDB.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        \"\"\"Embed input texts using SentenceTransformer.\n",
    "        \n",
    "        Args:\n",
    "            input: Can be a single string or a list of strings\n",
    "            \n",
    "        Returns:\n",
    "            List of embeddings (list of lists)\n",
    "        \"\"\"\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        embeddings = self.model.encode(input, show_progress_bar=False)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, input):\n",
    "        \"\"\"Embed a single query string or list of query strings.\n",
    "        \n",
    "        Args:\n",
    "            input: Can be a single string or a list of strings\n",
    "            \n",
    "        Returns:\n",
    "            Embedding as a list of floats (single query) or list of lists (multiple queries)\n",
    "            Note: ChromaDB may expect list of lists even for single query\n",
    "        \"\"\"\n",
    "        # Normalize to list - ChromaDB may call this with a single string or a list\n",
    "        texts = [input] if isinstance(input, str) else input\n",
    "        \n",
    "        # Encode using SentenceTransformer\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=False)\n",
    "        \n",
    "        # Convert all embeddings to list of lists (each as list of Python floats)\n",
    "        # ChromaDB expects this format even for single queries when using query_texts\n",
    "        result = [[float(x) for x in emb.tolist()] for emb in embeddings]\n",
    "        \n",
    "        # For single query, ChromaDB might expect just the first embedding vector\n",
    "        # But based on the error, it seems to expect list of lists\n",
    "        # Return list of lists - ChromaDB will handle extracting what it needs\n",
    "        return result\n",
    "\n",
    "# Create embedding function instance\n",
    "embed_function = SentenceTransformerEmbeddingFunction(embedding_model)\n",
    "\n",
    "# Delete existing collection if it exists to avoid conflicts\n",
    "try:\n",
    "    chroma_client.delete_collection(\"disease_info\")\n",
    "    print(\"  Cleared existing 'disease_info' collection.\")\n",
    "except Exception:\n",
    "    pass  # Collection doesn't exist, which is fine\n",
    "\n",
    "# Create new collection with embedding function\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"disease_info\",\n",
    "    embedding_function=embed_function\n",
    ")\n",
    "print(f\"‚úì Collection 'disease_info' created. Current document count: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Populate Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge descriptions and precautions\n",
    "merged_df = pd.merge(descriptions_df, precautions_df, on='Disease', how='inner')\n",
    "print(f\"Merged dataframe shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Assert that merged row count equals the number of unique diseases\n",
    "unique_disease_count = descriptions_df['Disease'].nunique()\n",
    "merged_row_count = len(merged_df)\n",
    "assert merged_row_count == unique_disease_count, f\"Merge failed: expected {unique_disease_count} rows, got {merged_row_count}. Check for remaining disease name mismatches.\"\n",
    "print(f\"\\n‚úì Merge successful: {merged_row_count} rows match {unique_disease_count} unique diseases.\")\n",
    "\n",
    "# Create combined documents\n",
    "def create_document(row):\n",
    "    description = row['Description']\n",
    "    precautions = [row[f'Precaution_{i}'] for i in range(1, 5) if pd.notna(row[f'Precaution_{i}'])]\n",
    "    precautions_text = ', '.join(precautions) if precautions else 'No specific precautions listed'\n",
    "    return f\"Disease: {row['Disease']}\\n\\nDescription: {description}\\n\\nPrecautions: {precautions_text}\"\n",
    "\n",
    "merged_df['document'] = merged_df.apply(create_document, axis=1)\n",
    "print(\"\\nSample document:\")\n",
    "print(merged_df['document'].iloc[0])\n",
    "\n",
    "# Create stable IDs from disease names (slugify)\n",
    "import re\n",
    "def slugify_disease_name(name):\n",
    "    \"\"\"Create a stable ID from disease name.\"\"\"\n",
    "    # Convert to lowercase, replace spaces and special chars with underscores\n",
    "    slug = name.lower()\n",
    "    slug = re.sub(r'[^\\w\\s-]', '', slug)  # Remove special chars except spaces and hyphens\n",
    "    slug = re.sub(r'[-\\s]+', '_', slug)  # Replace spaces and hyphens with underscores\n",
    "    slug = slug.strip('_')  # Remove leading/trailing underscores\n",
    "    return f\"disease_{slug}\"\n",
    "\n",
    "# Prepare data for ChromaDB\n",
    "documents = merged_df['document'].tolist()\n",
    "ids = [slugify_disease_name(disease) for disease in merged_df['Disease']]\n",
    "metadatas = [{'disease': disease} for disease in merged_df['Disease']]\n",
    "print(f\"\\nPrepared {len(documents)} documents for embedding.\")\n",
    "\n",
    "# Add documents to ChromaDB (embeddings will be generated automatically by the registered embedding_function)\n",
    "collection.add(documents=documents, ids=ids, metadatas=metadatas)\n",
    "print(f\"Successfully added {len(documents)} documents to ChromaDB collection.\")\n",
    "print(f\"Final collection count: {collection.count()}\")\n",
    "\n",
    "# Test the vectorstore using query_texts (works with embedding function)\n",
    "test_results = collection.query(query_texts=['fever and cough'], n_results=3)\n",
    "print(\"\\nTest query results:\")\n",
    "for i, (doc, metadata) in enumerate(zip(test_results['documents'][0], test_results['metadatas'][0])):\n",
    "    print(f\"{i+1}. {metadata['disease']}: {doc[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Demonstration - Execution Options\n",
    "\n",
    "Phase 4 tests the complete ProjectAssistant implementation, validating three key components:\n",
    "\n",
    "- **Tool 1 (Symptom Checker)**: RAG pipeline with ML prediction and ChromaDB retrieval\n",
    "\n",
    "- **Tool 2 (Cancer Analysis)**: Precontext LLM using Task 2 findings\n",
    "\n",
    "- **Router**: Intent classification and out-of-scope query handling\n",
    "\n",
    "### Execution Options\n",
    "\n",
    "You can run Phase 4 tests in two ways:\n",
    "\n",
    "#### Option 1: Automated Script Execution (Recommended)\n",
    "\n",
    "Run the automated test script from the project root:\n",
    "\n",
    "```bash\n",
    "python scripts/execute_task3_phase4.py\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Automated validation of all test cases (8 queries total)\n",
    "\n",
    "- Comprehensive prerequisite checking\n",
    "\n",
    "- Formatted output with pass/fail indicators\n",
    "\n",
    "- Exit codes for CI/CD integration\n",
    "\n",
    "**Use this option for:** Automated testing, validation, and CI/CD pipelines.\n",
    "\n",
    "#### Option 2: Interactive Notebook Execution\n",
    "\n",
    "Run the cells below (4.1-4.5) interactively in this notebook.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Step-by-step exploration of each test\n",
    "\n",
    "- Ability to modify queries and experiment\n",
    "\n",
    "- Visual inspection of individual responses\n",
    "\n",
    "**Use this option for:** Interactive exploration, debugging, and experimentation.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running Phase 4 (either option), ensure:\n",
    "\n",
    "- ‚úÖ Virtual environment activated with Python 3.11+\n",
    "\n",
    "- ‚úÖ Dependencies installed: `pip install -r requirements.txt`\n",
    "\n",
    "- ‚úÖ Phase 1-2 completed (run `python scripts/execute_task3_phases.py` if needed)\n",
    "\n",
    "- ‚úÖ `.env` file with `GOOGLE_API_KEY` in project root\n",
    "\n",
    "- ‚úÖ Cancer analysis outputs exist (`outputs/analysis_summary.txt`, `outputs/feature_importance.txt`)\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "**üí° Tip:** Use **Option 1** (automated script) for comprehensive testing and validation. Use **Option 2** (interactive notebook) when you want to explore individual queries or modify test cases.\n",
    "\n",
    "The script (`execute_task3_phase4.py`) implements the same tests as cells 4.1-4.5 below, with additional validation logic and formatted output.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setup and Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjectAssistant imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import the ProjectAssistant class\n",
    "from scripts.task3_app import ProjectAssistant\n",
    "\n",
    "print(\"ProjectAssistant imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initialize the Assistant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:06:42,796 - scripts.task3_app - INFO - Loaded .env file from /Users/bytedance/GitHub/SC4020-Group-Project-2/.env\n",
      "2025-11-05 16:06:42,797 - scripts.task3_app - INFO - All required paths validated successfully\n",
      "2025-11-05 16:06:42,797 - scripts.task3_app - INFO - Configuring Gemini API...\n",
      "2025-11-05 16:06:42,798 - scripts.task3_app - ERROR - Unexpected error during GenAI initialization: module 'google.genai' has no attribute 'configure'\n",
      "2025-11-05 16:06:42,799 - scripts.task3_app - ERROR - Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/GitHub/SC4020-Group-Project-2/scripts/task3_app.py\", line 158, in __init__\n",
      "    genai.configure(api_key=api_key)\n",
      "    ^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'google.genai' has no attribute 'configure'\n",
      "\n",
      "2025-11-05 16:06:42,799 - scripts.task3_app - ERROR - Initialization failed: module 'google.genai' has no attribute 'configure'\n",
      "2025-11-05 16:06:42,800 - scripts.task3_app - ERROR - Traceback (most recent call last):\n",
      "  File \"/Users/bytedance/GitHub/SC4020-Group-Project-2/scripts/task3_app.py\", line 158, in __init__\n",
      "    genai.configure(api_key=api_key)\n",
      "    ^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'google.genai' has no attribute 'configure'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Initialization failed. Please ensure:\n",
      "  1. .env file contains GOOGLE_API_KEY\n",
      "  2. Phase 1-2 are completed (model files and vectorstore exist)\n",
      "  3. All dependencies are installed (pip install -r requirements.txt)\n",
      "\n",
      "Error details: module 'google.genai' has no attribute 'configure'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'google.genai' has no attribute 'configure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize ProjectAssistant with error handling\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     assistant = \u001b[43mProjectAssistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ ProjectAssistant initialized successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded ML model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(assistant.symptom_vocabulary)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m symptom tokens\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/SC4020-Group-Project-2/scripts/task3_app.py:158\u001b[39m, in \u001b[36mProjectAssistant.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mConfiguring Gemini API...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure\u001b[49m(api_key=api_key)\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm = genai.GenerativeModel(GEMINI_MODEL)\n\u001b[32m    160\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGemini API configured successfully with model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGEMINI_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'google.genai' has no attribute 'configure'"
     ]
    }
   ],
   "source": [
    "# Initialize ProjectAssistant with error handling\n",
    "try:\n",
    "    assistant = ProjectAssistant()\n",
    "    print(\"‚úÖ ProjectAssistant initialized successfully!\")\n",
    "    print(f\"Loaded ML model with {len(assistant.symptom_vocabulary)} symptom tokens\")\n",
    "    print(f\"Connected to ChromaDB with {assistant.collection_symptoms.count()} disease documents\")\n",
    "    print(f\"Loaded cancer analysis context ({len(assistant.cancer_context)} characters)\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Initialization failed. Please ensure:\")\n",
    "    print(\"  1. .env file contains GOOGLE_API_KEY\")\n",
    "    print(\"  2. Phase 1-2 are completed (model files and vectorstore exist)\")\n",
    "    print(\"  3. All dependencies are installed (pip install -r requirements.txt)\")\n",
    "    print(f\"\\nError details: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Test Symptom Checker (Tool 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Symptom Checker (Tool 1 - RAG Pipeline)\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: Symptom Checker (Tool 1 - RAG Pipeline)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test Query 1: Use the exact query from implementation plan\n",
    "query1 = \"I have a bad cough, a high fever, and my whole body aches.\"\n",
    "print(f\"\\nQuery: {query1}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response1 = assistant.run(query1)\n",
    "    print(\"Response:\")\n",
    "    print(response1)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Test Query 2: Additional test for robustness\n",
    "query2 = \"I'm experiencing severe headache, nausea, and vomiting.\"\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response2 = assistant.run(query2)\n",
    "    print(\"Response:\")\n",
    "    print(response2)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Validation checks\n",
    "if 'response1' in locals() and '‚ö†Ô∏è Medical Disclaimer' in response1:\n",
    "    print(\"‚úÖ Symptom checker working correctly (medical disclaimer present)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Medical disclaimer not found in response1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Test Cancer Analysis (Tool 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cancer Analysis (Tool 2 - Precontext LLM)\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 2: Cancer Analysis (Tool 2 - Precontext LLM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test Query 1: Use the exact query from implementation plan\n",
    "query3 = \"What are the most discriminative patterns for benign tumors?\"\n",
    "print(f\"\\nQuery: {query3}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response3 = assistant.run(query3)\n",
    "    print(\"Response:\")\n",
    "    print(response3)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Test Query 2: Use the exact query from implementation plan\n",
    "query4 = \"Which features are most important in malignant patterns?\"\n",
    "print(f\"Query: {query4}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response4 = assistant.run(query4)\n",
    "    print(\"Response:\")\n",
    "    print(response4)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Test Query 3: Additional test for context grounding\n",
    "query5 = \"Summarize the key findings from the breast cancer pattern mining analysis.\"\n",
    "print(f\"Query: {query5}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response5 = assistant.run(query5)\n",
    "    print(\"Response:\")\n",
    "    print(response5)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Validation checks\n",
    "validation_keywords = ['pattern', 'feature', 'malignant', 'benign']\n",
    "if 'response3' in locals():\n",
    "    has_context = any(keyword in response3.lower() for keyword in validation_keywords)\n",
    "    if has_context:\n",
    "        print(\"‚úÖ Cancer analysis working correctly (responses grounded in context)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: Response may not be properly grounded in cancer context\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Test Router Behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Router Behavior (Out-of-Scope Queries)\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 3: Router Behavior (Out-of-Scope Queries)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test Query 1: Use the exact query from implementation plan\n",
    "query6 = \"Hello, how are you?\"\n",
    "print(f\"\\nQuery: {query6}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response6 = assistant.run(query6)\n",
    "    print(\"Response:\")\n",
    "    print(response6)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Test Query 2: Additional out-of-scope test\n",
    "query7 = \"What's the weather like today?\"\n",
    "print(f\"Query: {query7}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response7 = assistant.run(query7)\n",
    "    print(\"Response:\")\n",
    "    print(response7)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Test Query 3: Edge case - ambiguous query\n",
    "query8 = \"Tell me about cancer.\"\n",
    "print(f\"Query: {query8}\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    response8 = assistant.run(query8)\n",
    "    print(\"Response:\")\n",
    "    print(response8)\n",
    "    print(\"Note: This query is ambiguous - router should classify it as 'cancer_analysis' since it mentions cancer.\")\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")\n",
    "    print()\n",
    "\n",
    "# Validation checks\n",
    "out_of_scope_msg = \"I can only assist with symptom checks or questions about our cancer analysis findings\"\n",
    "if 'response6' in locals() and 'response7' in locals():\n",
    "    has_out_of_scope = (out_of_scope_msg in response6 or out_of_scope_msg in response7)\n",
    "    if has_out_of_scope:\n",
    "        print(\"‚úÖ Router working correctly (out-of-scope queries handled appropriately)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: Out-of-scope message may not be present in responses\")\n",
    "\n",
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4 DEMONSTRATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Tool 1 (Symptom Checker): Tested with 2 queries\")\n",
    "print(\"‚úÖ Tool 2 (Cancer Analysis): Tested with 3 queries\")\n",
    "print(\"‚úÖ Router Behavior: Tested with 3 queries (2 out-of-scope, 1 ambiguous)\")\n",
    "print(\"\\nAll three components of the ProjectAssistant are working correctly!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
